{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There already exists an environment generator that will make and wrap atari environments correctly.\n",
    "env = make_atari_env(\"PongNoFrameskip-v4\", n_envs=4, seed=0)\n",
    "# Stack 4 frames\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(\n",
    "  \"CnnPolicy\",\n",
    "  env,\n",
    "  verbose=1,  \n",
    "  learning_rate=0.0005,  # Adjust learning rate  \n",
    "  n_steps=20,  # Increase steps taken in each rollout\n",
    "  gamma=0.99,  # Adjust discount factor\n",
    "  gae_lambda=0.95,  # Adjust factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "  ent_coef=0.01,  # Adjust entropy coefficient\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=25_000)\n",
    "# Save the agent\n",
    "model.save(\"A2C_atari\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
